[
  {
    "objectID": "Speaker.html",
    "href": "Speaker.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "ğŸ”¹ ShinyConf 2025\n\nTitle: Reviewing Clinical Data Efficiently with Shiny\nğŸ“ Date: 2025-04-12\nğŸ“ Summary:\nThis presentation introduces a Shiny-based application designed to improve the efficiency of clinical data review. Traditional EDC systems often limit reviewers to viewing data one form and one patient at a time, making it difficult to cross-reference information across forms such as Adverse Events (AE), Exposure (EX), and Concomitant Medication (CM). This tool addresses that challenge by providing a user-friendly, click-driven interface that allows reviewers to select patients, filter forms and variables, and instantly visualize clinical timelines and data listings. The application maintains the original data structure, requires minimal setup by programmers, and is accessible to non-programming users. Key benefits include simultaneous multi-form data review, integrated visualizations and listings, and Excel export functionality. This tool aims to bridge communication gaps between reviewers and programmers while enhancing the speed and clarity of clinical review workflows..\nğŸ“„ Download Slides (PDF)\n\n\n\nğŸ”¹ R/Pharma 2024\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\nğŸ“ Date: 2024-10-31\nğŸ“ Summary:\nThis presentation explores how R and Shiny can enhance the review and visualization of clinical trial data. Traditional workflows often involve repeated back-and-forth verification between datasets such as SDTM, ADaM, and EDC, which is time-consuming. By leveraging Râ€™s Shiny framework, we can streamline data filtering and visualization, enabling faster and more intuitive review processes for both statisticians and medical teams. The talk highlights three key Shiny applications: tumor response visualization, patient milestone tracking, and SDTM domain review. These tools support both population-level summaries and individual-level insights. Emphasis is also placed on the importance of using CDISC-compliant data formats to standardize and simplify data handling. Finally, the integration of Shiny with Quarto is introduced as a future direction to make clinical data more accessible to non-programmers, improving data transparency and efficiency in reporting.\nğŸ“„ Download Slides (PDF)\n\n\n\nğŸ”¹ Pharmasug 2018\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\nğŸ“ Date: 2018-08-31\nğŸ“ Summary:\nThis paper discusses how to enhance programming quality and efficiency in clinical trials under tight deadlines, especially within CROs. It emphasizes the importance of â€œfirst-time quality,â€ defined as the quality of deliverables before QC review. The author argues that poor initial quality leads to higher correction costs and delays, despite common practices like SOPs, validated macros, and training. A structured QC plan is essential but resource-intensive. The paper highlights factors affecting quality, including unfamiliarity with study design, insufficient task understanding, and client complexity. To address this, the author proposes a â€œFirst-Time Quality Scaleâ€ that evaluates programmers based on QC comments, client feedback, and satisfaction. High-scoring individuals can be assigned to time-sensitive projects, maximizing both quality and speed. The paper concludes that focusing on first-time quality can reduce repetitive work, save resources, and improve overall outcomes, aligning with the increasing demand for rapid and accurate clinical data processing.\nğŸ“„ Download Article (PDF)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Plot-Table Highlighting in Shiny\n\n\n\nR\n\nDT\n\nShiny\n\nplotly\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\nCollaborating with ChatGPT in Coding\n\n\n\nAI\n\nChatGPT\n\nProgramming\n\n\n\nWhat Iâ€™ve Learned So Far\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\nğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ\n\n\n\nR\n\nhash\n\nZodiac\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\nGenerate Dynamic Text Results with glue and lapply in R\n\n\n\nR\n\nTidyverse\n\nglue\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nEfficiently Apply the Same Function to Multiple Datasets in R\n\n\n\nR\n\nlapply\n\ngsub\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "href": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "title": "Collaborating with ChatGPT in Coding",
    "section": "",
    "text": "Lately, Iâ€™ve been exploring how to work more effectively with ChatGPT when writing codeâ€”mostly in R and Pythonâ€”to boost my productivity and reach my goals faster.\nTo be honest, it hasnâ€™t always been smooth sailing. While AI can be a powerful assistant, working with it efficiently takes practice and intention. Iâ€™m still learning and fine-tuning my process, but Iâ€™ve picked up a few lessons along the way that Iâ€™d like to share. And of course, Iâ€™d love to hear your thoughts too ğŸª‡\nğŸ’¡ 1. Define the Programming Goal Clearly Before asking AI for help, Iâ€™ve found it crucial to clearly explain what Iâ€™m trying to achieve. The more specific and outcome-oriented the request, the more helpful the response. When I start with a well-defined goal, ChatGPT can often propose a clean structure or even a solid template for the task.\nğŸ’¡ 2. Understand the AI-Generated Code This might sound obvious, but itâ€™s tempting to copy and paste without fully understanding the AIâ€™s output. In reality, taking the time to read and grasp the logic is essential. It helps me make sure the code aligns with my intentâ€”and gives me a much better chance at troubleshooting if something goes wrong later on.\nğŸ’¡ 3. Debug Step by Step Initially, I asked ChatGPT to revise big chunks of code at once, but that often led to confusion. Iâ€™ve since learned that breaking things down into smaller parts works much better. By reviewing and applying changes step by step, I stay in control and reduce the risk of introducing new errors. It also allows me to better evaluate the AIâ€™s reasoning behind each suggestion.\nğŸ’¡ 4. Restart If the Conversation Gets Stuck There are times when ChatGPT just doesnâ€™t seem to â€œget itâ€â€”no matter how I phrase my question. In those cases, Iâ€™ve found it helpful to summarize the issue clearly and start a fresh conversation. A clean slate often results in clearer, more accurate responses.\nğŸš§ Still a Work in Progress Iâ€™m still experimenting, reflecting, and learning through this process. For me, working with AI has been more than just about writing codeâ€”itâ€™s been a way to sharpen how I think and communicate as a developer.\nIf youâ€™ve had similar experiencesâ€”or totally different onesâ€”Iâ€™d love to hear how you collaborate with AI when coding. Letâ€™s share ideas and make this learning journey smarter, together. ğŸ”ğŸ”ğŸ”"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ§© Introduction",
    "text": "ğŸ§© Introduction\nWhen presenting analysis results, itâ€™s often necessary to embed specific values into structured sentences â€” for example, describing results for certain patients, countries, or hospitals.\nThe glue package in R provides a powerful solution for this. It allows you to integrate variable values into a fixed text template, making your code more efficient, readable, and less prone to errors.\nIn this post, Iâ€™ll demonstrate how to: - Use glue() to dynamically create descriptive text\n- Use glue_collapse() to collapse grouped records\n- Use lapply() to manage and prefix multiple datasets\n- Integrate the results into Quarto or R Markdown reports"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ“¦ Required Packages",
    "text": "ğŸ“¦ Required Packages\nlibrary(dplyr) \nlibrary(glue) \nğŸª‡ Step 1: Create and Prefix Multiple Datasetsrepresenting different hospitalsâ€™ medication records:\n# Sample CM data from different hospitals\ncm1 &lt;- data.frame(USUBJID = c(\"001\", \"002\"), CMTRT = c(\"DrugA\", \"DrugB\"))\ncm2 &lt;- data.frame(USUBJID = c(\"003\", \"004\"), CMTRT = c(\"DrugC\", \"DrugD\"))\n\n# Put into a named list\ncm_list &lt;- list(Hosp1 = cm1, Hosp2 = cm2)\n\n# Prefix each dataset with its group name\ncm_prefixed &lt;- lapply(names(cm_list), function(name) {\n  df &lt;- cm_list[[name]]\n  df$Group &lt;- name\n  df\n})\n\n# Merge all into one\ncm_all &lt;- bind_rows(cm_prefixed)"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ§  Step 2: Collapse Records per Subject",
    "text": "ğŸ§  Step 2: Collapse Records per Subject\nWe want to describe each subjectâ€™s treatment history by combining multiple CMTRT values:\n# Example: combine multiple treatments per subject\ncm_text &lt;- cm_all %&gt;%\n  group_by(USUBJID, Group) %&gt;%\n  summarise(cmx_CMTRT = glue_collapse(CMTRT, sep = \"; \"), .groups = \"drop\")\nâœ¨ Step 3: Use glue() to Format Sentences\n# Create dynamic sentences for reporting\ncm_text &lt;- cm_text %&gt;%\n  mutate(sentence = glue(\"Subject {USUBJID} in {Group} was treated with: {cmx_CMTRT}.\"))\n\n# Preview\ncm_text$sentence\nğŸ“ Output Example\nSubject 001 in Hosp1 was treated with: DrugA.\nSubject 002 in Hosp1 was treated with: DrugB.\nSubject 003 in Hosp2 was treated with: DrugC.\nSubject 004 in Hosp2 was treated with: DrugD.\nğŸ§µ Conclusion\nThe combination of glue(), glue_collapse(), and lapply() offers a powerful workflow for:\n\nfficient text generation\nFlexible dataset processing\nClean report integration\n\nThis approach not only reduces manual effort but also ensures consistency and clarity in reporting."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "",
    "text": "The Lunar New Year holiday has begun â€” Happy Year of the Snake! ğŸ‰ğŸğŸ‰\nIn many Eastern cultures, the zodiac (ç”Ÿè‚–) plays an important role in tradition, storytelling, and even personal identity. This 12-year cycle assigns an animal to each year, with the animal of your birth year becoming your zodiac sign. For example, those born in 1989 or 2001 are Snakes ğŸ, while others may be Tigers ğŸ… or Rabbits ğŸ‡.\nBut how exactly can we figure out someoneâ€™s zodiac animal using code?\nSure, you could just Google itâ€¦ ğŸ˜‚\nBut as an R programmer, I decided to use this as a small coding challenge!"
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ” The Idea",
    "text": "ğŸ” The Idea\nWeâ€™ll base our logic on the Common Era (CE) calendar, and apply a combination of vectors and the {hash} package in R to map a given year to its corresponding zodiac animal.\n\nğŸ’¡ Note: For more precise results, especially if youâ€™re working with historical or cultural data, you might want to factor in the lunar calendar (which usually starts in late January or early February). For this example, weâ€™ll keep things simple."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ§® How the Chinese Zodiac Works",
    "text": "ğŸ§® How the Chinese Zodiac Works\nThe 12 animals, in order, are:\n\nRat\n\nOx\n\nTiger\n\nRabbit\n\nDragon\n\nSnake\n\nHorse\n\nGoat\n\nMonkey\n\nRooster\n\nDog\n\nPig\n\nThe cycle repeats every 12 years. For instance, the year 2025 will be the Year of the Snake, just like 2013, 2001, 1989, etc."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ’» Coding It in R",
    "text": "ğŸ’» Coding It in R\nLetâ€™s write a simple function using the {hash} package:\n# Load the hash package library(hash)"
  },
  {
    "objectID": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "href": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "title": "Plot-Table Highlighting in Shiny",
    "section": "",
    "text": "When working with large volumes of clinical trial data, itâ€™s easy to get lost in hundredsâ€”or even thousandsâ€”of records. To support more intuitive data review, Iâ€™ve recently been exploring ways to combine visual plots with interactive data tables in Shiny. This approach can help reviewers quickly grasp key insights and trace them back to the raw data.\nOne idea I tested recently is a â€œhighlight recordâ€ function, which links points on a plot to specific rows in a data listing. This functionality is built using three powerful R packages: {ggplot2}, {plotly}, and {DT}.\nğŸ” How It Works: Interactive Highlighting Step-by-Step Hereâ€™s a breakdown of the mechanism:\n\nğŸ•¯ï¸ Step 1: Create the Base Plot with ggplot2\nThe visualization starts with a standard ggplot chartâ€”for example, plotting subject-level events by date and domain. This gives us full control over the aesthetics and data structure.\n\n\nğŸ•¯ï¸ Step 2: Make the Plot Interactive with plotly\nNext, I use ggplotly() from the {plotly} package to convert the static ggplot into an interactive chart. With this transformation, the plot becomes clickable and dynamic, enabling deeper user interaction.\n\n\nğŸ•¯ï¸ Step 3: Capture User Clicks Using event_data(â€œplotly_clickâ€)\nThanks to plotly_click event data, I can capture exactly which point a user clicks onâ€”such as the domain, event date, or subject ID. This click event generates metadata we can use to match against the underlying dataset.\n\n\nğŸ•¯ï¸ Step 4: Highlight the Matched Record in DT::datatable()\nThe final step is to link the clicked point to a specific row in the data table rendered with the {DT} package. When a match is found (based on selected key values like date and domain), the corresponding row is automatically highlighted, drawing the reviewerâ€™s attention to the source record.\n\n\nğŸš€ Why This Matters\nThis interaction model significantly improves the reviewer experience:\nVisual-first exploration: Users can spot patterns and anomalies visually.\nSeamless data tracing: Clicking on a point takes you straight to the corresponding recordâ€”no need to scroll through the full table.\nFaster reviews: Especially useful when reviewing patient timelines, safety events, or domain-specific findings.\n\n\nâš ï¸ One Caveat: Handle Factor Conversion with Care\nOne challenge I encountered involves the matching logic. Since ggplot may internally convert character variables (like domain) into factors, you must be very cautious when comparing click event values to original data values.\nA mismatchâ€”say, due to different data types or formattingâ€”can easily break the highlight feature. This is a common area for debugging, especially when your plot aesthetics depend on factor() transformations or custom labeling.\n\n\nFinal Thoughts\nThis approach is still evolving, but the integration of plot interactivity and table linking is already proving to be a valuable enhancement for Shiny apps, especially in clinical data review contexts. If youâ€™re working with multi-domain datasets or timeline-based visualizations, this pattern might be worth exploring. If you have built something similar or have ideas to improve this logic, I would love to hear your thoughts! â€”"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "href": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ” Introduction",
    "text": "ğŸ” Introduction\nIn data analysis, we often encounter situations where we need to perform the same transformation or calculation on multiple datasets â€” such as datasets grouped by age, treatment, or study phase.\nRather than repeating the same code block over and over, R provides a more efficient, less error-prone solution: the lapply() function."
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "href": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "âœ¨ Why Use lapply()?",
    "text": "âœ¨ Why Use lapply()?\nWhen you have more than 5 or even 10 datasets to process, using a loop or manually running the same function can quickly become tedious and risky. By placing all target datasets in a list and applying a custom function via lapply(), you can:\n\nMinimize code repetition\n\nReduce the chance of mistakes\n\nImprove code scalability and clarity"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "href": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ”§ Example: Adjust Units and Calculate BMI",
    "text": "ğŸ”§ Example: Adjust Units and Calculate BMI\nLetâ€™s say you have datasets containing weight (in å…¬æ–¤) and height (in å…¬å°º) for different age groups. The goal is to:\n\nConvert the units from Chinese characters to standard abbreviations (kg, m)\n\nCalculate the Body Mass Index (BMI) using the formula:\n\n\\[\n\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\n\\]\n\nğŸ“¦ Step 1: Create Sample Datasets\n# Sample data for three age groups\ngroup1 &lt;- data.frame(Height = c(\"1.65å…¬å°º\", \"1.70å…¬å°º\"), Weight = c(\"60å…¬æ–¤\", \"65å…¬æ–¤\"))\n\ngroup2 &lt;- data.frame(Height = c(\"1.60å…¬å°º\", \"1.75å…¬å°º\"), Weight = c(\"55å…¬æ–¤\", \"70å…¬æ–¤\"))\n\ngroup3 &lt;- data.frame(Height = c(\"1.72å…¬å°º\", \"1.80å…¬å°º\"), Weight = c(\"68å…¬æ–¤\", \"75å…¬æ–¤\"))\n# Combine them into a list\ngroups &lt;- list(group1, group2, group3)\nğŸ”§ Step 2: Define a Custom Function\n# Function to convert units and calculate BMI\nreplace_function &lt;- function(df) {\n  df$Height &lt;- as.numeric(gsub(\"å…¬å°º\", \"\", df$Height))  # Remove 'å…¬å°º' and convert to numeric\n  df$Weight &lt;- as.numeric(gsub(\"å…¬æ–¤\", \"\", df$Weight))  # Remove 'å…¬æ–¤' and convert to numeric\n  df$BMI &lt;- round(df$Weight / (df$Height^2), 1)         # Calculate BMI\n  return(df)\n}\nğŸš€ Step 3: Apply the Function Using lapply()\n# Apply the same function to all groups\nresults &lt;- lapply(groups, replace_function)\n\n# Preview the result for group1\nresults[[1]]\nâœ… Output (example)\n  Height Weight  BMI\n1   1.65     60  22.0\n2   1.70     65  22.5"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "href": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ’¡ Final Thoughts",
    "text": "ğŸ’¡ Final Thoughts\nThe combination of lapply() and gsub() demonstrates a powerful pattern in R: clean, consistent, and reproducible operations across datasets.\nWhether youâ€™re dealing with different demographic groups or multiple study arms, putting your datasets in a list and defining a reusable function can save time and prevent mistakes â€” especially in clinical trial data preparation or large-scale reporting tasks.\nHappy coding!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Winkle Lu",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n     winklelu1226@gmail.com\n  \n\n      \nWith over a decade of experience in clinical trial programming, I specialize in CDISC standards and regulatory deliverable â€” but Iâ€™m not standing still. Iâ€™ve embraced open-source tools like R, Shiny, and Python to drive automation and improve data visualization.\nğŸ“˜ Blog/Sharing | ğŸ‘‰ Presentations\n\n\nClinical Research Organization | Pharmaceutical Company | Statistical Programming\n\n\n\nMaster of Public Health | Tzu Chi University\n\n\n\n\n11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for.\n\n\n\n\n\nAllergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis\n\n\n\n\n\nWinkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time â€“ discussion by view of first-time quality, PharmaSUG â€“ Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428â€“435 [SCI]."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Winkle Lu",
    "section": "",
    "text": "Clinical Research Organization | Pharmaceutical Company | Statistical Programming"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Winkle Lu",
    "section": "",
    "text": "Master of Public Health | Tzu Chi University"
  },
  {
    "objectID": "index.html#performance-summary",
    "href": "index.html#performance-summary",
    "title": "Winkle Lu",
    "section": "",
    "text": "11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for."
  },
  {
    "objectID": "index.html#therapeutic-area",
    "href": "index.html#therapeutic-area",
    "title": "Winkle Lu",
    "section": "",
    "text": "Allergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Winkle Lu",
    "section": "",
    "text": "Winkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time â€“ discussion by view of first-time quality, PharmaSUG â€“ Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428â€“435 [SCI]."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«-what-is-a-statistical-programmer-sp",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«-what-is-a-statistical-programmer-sp",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ” ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸« | What is a Statistical Programmer (SP)",
    "text": "ğŸ” ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸« | What is a Statistical Programmer (SP)\nåœ¨è‡¨åºŠè©¦é©—çš„éç¨‹ä¸­ï¼Œçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«è² è²¬å°‡è©¦é©—æ•¸æ“šä¾ç…§è¦ç¯„æ•´ç†æˆå¯ä»¥è§£é‡‹ä¸”é©åˆå ±å‘Šå‘ˆç¾çš„æ ¼å¼ã€‚é€™è£¡æ‰€èªªçš„ã€Œè¦ç¯„ã€ï¼ŒåŒ…å«å¯©æŸ¥ä¸»ç®¡æ©Ÿé—œï¼ˆä¾‹å¦‚ï¼šFDAã€EMAï¼‰æ‰€è¦æ±‚çš„æäº¤è¦æ ¼ã€CDISC åˆ¶å®šçš„æ¨™æº–ï¼ˆå¦‚ SDTMã€ADaMã€define.xmlï¼‰ï¼Œä»¥åŠå…¬å¸å…§éƒ¨çš„ä½œæ¥­è¦ç¯„ã€‚\nIn clinical trials, Statistical Programmers are responsible for organizing trial data according to predefined standards, transforming it into interpretable and report-ready formats. These â€œstandardsâ€ include regulatory submission requirements (such as those from the FDA and EMA), CDISC-defined structures like SDTM, ADaM, and define.xml, as well as internal company-specific guidelines.\né€™å€‹å•é¡Œï¼Œå…¶å¯¦æ˜¯èº«ç‚ºçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«å‘èº«æ—è¦ªå‹è§£é‡‹å·¥ä½œå…§å®¹æ™‚ï¼Œæœ€é›£èªªæ˜çš„ä¸€éƒ¨åˆ†ï¼Œå› ç‚ºå¾ˆå®¹æ˜“èˆ‡ Data Managementã€Statistician ç­‰è·å‹™æ··æ·†ã€‚ä¸éä¹Ÿæ­£å› å¦‚æ­¤ï¼Œçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«èˆ‡ Data Managementã€Statistician çš„å·¥ä½œç¢ºå¯¦ç¶“å¸¸å¯†ä¸å¯åˆ†ã€‚\nThis is often one of the most difficult aspects for Statistical Programmers to explain to friends and family, as it is easily confused with roles like Data Management or Statistician. However, this overlap also highlights how closely intertwined these roles truly are.\næˆ‘åœ¨è‡¨åºŠè©¦é©—é ˜åŸŸæ“”ä»»çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«è¶…éåå¹´çš„æ™‚é–“ï¼Œå¾…éå…©é–“å¤§å‹ CRO å…¬å¸èˆ‡å…©é–“è—¥å» ã€‚ä»¥ä¸‹æˆ‘å°‡ç°¡å–®åˆ†äº«ä¸€äº›å¾è‡ªå·±è§’åº¦å‡ºç™¼ï¼Œå°é€™ä»½å·¥ä½œå…§å®¹èˆ‡åƒ¹å€¼çš„è§€å¯Ÿèˆ‡ç†è§£ã€‚\nI have worked as a Statistical Programmer in the clinical trial field for over ten years, across two large CRO companies and two pharmaceutical firms. In the following sections, Iâ€™ll share my perspective on the nature and value of this profession based on my own experiences."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-åœ¨åšä»€éº¼-what-does-a-statistical-programmer-actually-do",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-åœ¨åšä»€éº¼-what-does-a-statistical-programmer-actually-do",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ” Statistical Programmer åœ¨åšä»€éº¼ | What Does a Statistical Programmer Actually Do",
    "text": "ğŸ” Statistical Programmer åœ¨åšä»€éº¼ | What Does a Statistical Programmer Actually Do\nå…ˆå‰æ“”ä»»é¢è©¦ä¸»ç®¡çš„æ™‚å€™ï¼Œå¸¸æœƒè½åˆ°é¢è©¦è€…æåˆ° CDISCã€‚CDISC æ˜¯ä»€éº¼å‘¢ï¼Ÿç°¡å–®ä¾†èªªï¼Œæ˜¯é‡å°è‡¨åºŠæ•¸æ“šåˆ¶å®šç›¸é—œäº¤æ›èˆ‡æ•´ç†æ¨™æº–çš„çµ„ç¹”ã€‚è€Œå…¶ä¸­ SDTMã€ADaM çš„æ¨™æº–åŒ–è¦ç¯„èˆ‡å»ºè­°å°±æ˜¯ç”± CDISC åˆ¶å®šå‡ºä¾†çš„ã€‚é€™å…¶å¯¦å°±æ˜¯ SP æ ¸å¿ƒå·¥ä½œçš„é‡è¦æ¨™æº–ä¹‹ä¸€ã€‚\nWhen I previously served as an interview manager, I often heard candidates mention CDISC. So, what exactly is CDISC? Simply, it is an organization that defines data exchange and standardization frameworks for clinical trial data. The well-known SDTM and ADaM standards were established by CDISC - and these are among the key foundations of a Statistical Programmerâ€™s role.\nCDISC æ˜¯æˆ‘éå¸¸å–œæ­¡çš„å–®ä½ä¹‹ä¸€ï¼Œå› ç‚ºè‡¨åºŠåŸå§‹æ•¸æ“šå…¶å¯¦æ˜¯éå¸¸å¤šå…ƒçš„ã€‚é€™å€‹çµ„ç¹”é‡å°è‡¨åºŠè©¦é©—å„éšæ®µçš„æ•¸æ“šé€²è¡Œæ¨™æº–åŒ–æ•´ç†ï¼Œè®“è³‡æ–™åœ¨æ•´ç†ã€åˆ†æã€ä»¥åŠæäº¤æ–¹é¢æ›´å…·æ•ˆç‡ã€‚\nCDISC is one of my favorite organizations, because clinical raw data is inherently diverse and complex. By applying standardized structures to each phase of a clinical trial, CDISC enables much more efficient data organization, analysis, and regulatory submission.\n\nä»¥ä¸€èˆ¬çš„æµç¨‹ä¾†èªªï¼Œç•¶è‡¨åºŠæ•¸æ“šä¾ç…§ CRFï¼ˆCase Report Formï¼‰é€²è¡Œæ”¶é›†è‡³ EDCï¼ˆElectronic Data Captureï¼‰ç³»çµ±å¾Œï¼ŒSP å°‡ä¾ç…§æ¡ˆå­çš„é€²åº¦èˆ‡éœ€æ±‚ï¼Œé–‹å§‹é€²è¡Œ SDTM æ•¸æ“šé›†çš„ç¨‹å¼ç·¨å¯«ã€‚åŒæ™‚é–“ï¼Œä¹Ÿæœƒèˆ‡ç›¸é—œéƒ¨é–€ï¼Œä¾‹å¦‚çµ±è¨ˆèˆ‡é†«å­¸éƒ¨é–€ï¼Œç¢ºèªçµ±è¨ˆåˆ†æçš„å…§å®¹å¾Œï¼Œé–‹å§‹é€²è¡Œ ADaM åŠ TLFï¼ˆTable, Listing, Figureï¼‰çš„æº–å‚™ã€‚\nIn a general workflow, once clinical data is collected by CRFs (Case Report Forms) and entered into the EDC (Electronic Data Capture) system, SPs begin programming the SDTM datasets based on the project timeline and needs. At the same time, they collaborate with key departments - such as statisticians and medical reviewers - to confirm the analysis plan and begin developing ADaM datasets and generating TLFs (Tables, Listings, and Figures).\nå›æ†¶è‡ªå·±å‰›é€²é€™é ˜åŸŸæ™‚ï¼Œé‚£æ™‚å€™çš„éƒ¨é–€åˆ†å·¥ååˆ†ç´°ï¼ŒSDTM èˆ‡ ADaM+TLF æ˜¯ç”±ä¸åŒçš„ SP åœ˜éšŠè² è²¬ã€‚æˆ‘ä¸€é–‹å§‹æ˜¯åœ¨è² è²¬ ADaM+TLF çš„åœ˜éšŠï¼Œç•¶æ™‚ç¬¬ä¸€å€‹ä¸»è¦ä»»å‹™æ˜¯ ADLB æ•¸æ“šé›†ã€‚ç”±æ–¼æ•¸é‡é¾å¤§ã€åˆ†ææ–¹æ³•è¤‡é›œï¼Œç¨‹å¼æ¯æ¬¡åŸ·è¡Œéƒ½è¦è€—æ™‚ 4ï½5 å°æ™‚ã€‚\nLooking back to when I first entered the field, the departments were highly specialized - separate teams handled SDTM and ADaM+TLF. I started in the ADaM+TLF team, and my first major task was the ADLB (laboratory) dataset. Due to its size and the complexity of the analysis methods, each run of the program would take 4 to 5 hours.\nè¨˜å¾—é‚£æ™‚å€™åœ˜éšŠå¸¸å¸¸ä¸€èµ·æ™šé¤å¾Œåˆä¸€èµ·åŠ ç­ï¼Œå°±é€™æ¨£æŒçºŒäº†å¹¾å€‹æœˆï¼Œæœ€å¾Œé †åˆ©å®Œæˆ Sponsor çš„éœ€æ±‚ï¼Œæ¥è‘—ä¹Ÿæ¨é€²è‡³ define.xml çš„è£½ä½œã€‚å°ç•¶æ™‚å¹´è³‡é‚„ä¸åˆ°ä¸€å¹´çš„æˆ‘ä¾†èªªï¼Œèƒ½æ¥è§¸é€™æ¨£çš„ä»»å‹™å¯¦å±¬é›£å¾—ï¼Œå“ˆå“ˆã€‚\nI remember how the team often stayed late after dinner to keep working together. After several months, we successfully met the sponsorâ€™s expectations and moved on to preparing define.xml. At the time, I had less than one year of experience, so being involved in such tasks was rare - and rewarding - for a junior SP.\né›–ç„¶ç¬¬ä¸€å¹´çš„ SP ç”Ÿæ´»è®“æˆ‘åƒç›¡è‹¦é ­ï¼Œä¸éåœ¨ç•¶æ™‚ mentor å’Œåœ˜éšŠçš„å¸¶é ˜ä¸‹ï¼Œä¹Ÿè®“æˆ‘æ›´å¿«æ‰“ä¸‹ SP ç”Ÿæ¶¯çš„é‡è¦åŸºç¤ã€‚\nAlthough that first year as a Statistical Programmer was filled with challenges, the guidance of my mentor and the support of the team helped me quickly build a solid foundation in this career."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#sp-èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©—-statistical-programmers-in-todays-clinical-trials",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#sp-èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©—-statistical-programmers-in-todays-clinical-trials",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "âœ¨ SP èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©— | Statistical Programmers in Todayâ€™s Clinical Trials",
    "text": "âœ¨ SP èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©— | Statistical Programmers in Todayâ€™s Clinical Trials\nå‰›å…¥è¡Œæ™‚ï¼ŒSP å·¥ä½œå¤šæ•¸ä»¥ SAS ç‚ºä¸»è¦çš„å¸¸ç”¨ç¨‹å¼èªè¨€ã€‚SAS æ˜¯ä¸€é …éå¸¸ç©©å®šçš„å·¥å…·ï¼Œæˆ‘èªç‚ºæœ€å¼·å¤§çš„éƒ¨åˆ†æ˜¯å®ƒèƒŒå¾Œçš„æ”¯æŒå…¬å¸è³‡æºã€‚å³ä½¿é‡åˆ° SAS æœ¬èº«çš„æŠ€è¡“å•é¡Œï¼Œä¹Ÿæœƒæœ‰å°ˆæ¥­åœ˜éšŠå”åŠ©è§£æ±ºï¼›å¦å¤–ï¼Œç”±æ–¼é€™æ˜¯è‡¨åºŠè©¦é©—é ˜åŸŸé•·ä¹…ä»¥ä¾†æ‰€ä½¿ç”¨çš„èªè¨€ï¼Œå¯ä¾›åƒè€ƒçš„ç¨‹å¼ç¯„ä¾‹éå¸¸è±å¯Œï¼Œé€™ä¹Ÿæ˜¯è‡³ä»Š SAS æ“æœ‰ä¸å¯æ’¼å‹•åœ°ä½çš„åŸå› ä¹‹ä¸€ã€‚\nWhen I first entered the field, SAS was the dominant programming language for Statistical Programmers. It is a highly stable tool, and in my view, its greatest strength lies in the robust support from the company behind it. Even when technical issues arise, professional support teams are available to help. Moreover, as SAS has been widely used in clinical trials for decades, it offers an abundance of reference programs â€” one of the key reasons for its long-standing, unshakable position in the field,\nå°è±¡ä¸­ï¼Œå¾ 2018 å¹´é–‹å§‹ï¼Œæˆ‘é–‹å§‹æœ‰æ©Ÿæœƒåƒèˆ‡å…¬å¸å¤–çš„ç ”è¨æœƒï¼Œopen-source tool ç›¸é—œçš„åˆ†äº«å·²ç¶“å¾ˆå¤šï¼Œä¾‹å¦‚ï¼šRã€Pythonã€‚é€™æ˜¯éå¸¸å¥½çš„ç¾è±¡ï¼Œä»£è¡¨æ•´å€‹è‡¨åºŠè©¦é©—é ˜åŸŸæ­£åœ¨ä¸æ–·æ€è€ƒèˆ‡é€²æ­¥ã€‚å°æˆ‘ä¾†èªªï¼Œé€™äº›å·¥å…·ä¸æ‡‰è©²è¢«ç”¨ä¾†èˆ‡ SAS ç›¸äº’æ¯”è¼ƒæˆ–ç«¶çˆ­ï¼Œè€Œæ˜¯æä¾›çµ¦ä½¿ç”¨è€…ï¼ˆä¾‹å¦‚ SPï¼‰æ›´å¤šå”ä½œçš„å¯èƒ½æ€§èˆ‡æ€è€ƒçš„ç©ºé–“ã€‚\nI recall that starting around 2018, I had the opportunity to attend external conferences, and there were already many discussions around open-source tools like R and Python. This is a very positive trend - it shows that the clinical trial industry is continuously evolving and open to new ideas. Personally, I donâ€™t see these tools as competitors to SAS. Rather, they offer more collaborative options and new perspectives for users like Statistical Programmers."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨-sp-çš„åˆ©å™¨-what-are-the-current-must-have-skills-for-sps",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨-sp-çš„åˆ©å™¨-what-are-the-current-must-have-skills-for-sps",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ’¡ è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨ SP çš„åˆ©å™¨ | What Are the Current Must-Have Skills for SPs",
    "text": "ğŸ’¡ è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨ SP çš„åˆ©å™¨ | What Are the Current Must-Have Skills for SPs\nå¦‚æœç”±æˆ‘ä¾†å›ç­”ï¼Œæˆ‘æœƒåˆ—èˆ‰ä»¥ä¸‹å¹¾é»ï¼š\nIf I were to answer this question, here are the capabilities I consider essential:\nğŸ§©ç†è§£ç•¶ä¸‹æ‰€é€²è¡Œçš„ä»»å‹™: SP ç¶“æ‰‹è¨±å¤šä»»å‹™ä¹Ÿå¸¸æœƒè·Ÿå…¶ä»–éƒ¨é–€åˆä½œ, ç†è§£ç•¶ä¸‹ä»»å‹™çš„åŸç”±é™¤äº†å¯ä»¥å¹«åŠ©ç¶­æŒæ­£ç¢ºé”æˆç›®æ¨™å¤–, ä¹Ÿå¯ä»¥é©æ™‚æå‡ºæ›´åˆé©çš„è§£æ±ºæ–¹æ¡ˆ\nUnderstanding the task at hand: SPs handle various tasks and frequently collaborate with other departments. Understanding the purpose behind the current task not only helps ensure that the objectives are met correctly, but also allows SPs to propose more appropriate solutions when needed.\nğŸ§©ç¶­æŒé«˜å“è³ªçš„ç”¢å‡º: è‡¨åºŠè©¦é©—ç›´æ¥é—œä¹äººé«”ç”Ÿå‘½ï¼Œæ˜¯ä¸€ä»½éœ€è¦å¯©æ…å°å¾…çš„å·¥ä½œï¼›è‹¥å› å“è³ªä¸ä½³è€Œå¤šæ¬¡ä¿®æ”¹ï¼Œå°‡ç›´æ¥å½±éŸ¿æ¡ˆå­çš„æ•´é«”æ™‚ç¨‹\nDelivering high-quality outputs: Clinical trials involve human lives, and this work must be approached with great care. Poor quality that requires repeated revisions can directly delay project timelines.\nğŸ§©ç†Ÿæ‚‰å¸¸è¦å·¥ä½œçš„æŠ€èƒ½: äº†è§£æ¯å€‹å¸¸è¦æ­¥é©Ÿï¼Œä¸¦é«˜æ•ˆç‡å®Œæˆå·¥ä½œï¼Œæ˜¯é™ä½é¢¨éšªèˆ‡æå‡ç©©å®šæ€§çš„é—œéµ\nMastering routine tasks: Understanding each standard step and performing them efficiently helps reduce risk and ensure consistent deliverables.\nğŸ§©ç¶­æŒæ€è€ƒèˆ‡æ‹“å±•è¦–é‡: SP çš„å¸¸è¦ç¨‹å¼é–‹ç™¼éœ€æ±‚å±¬æ–¼ä¸­ç­‰ç¨‹åº¦ï¼Œä½†è‹¥èƒ½ä¸æ–·æ€è€ƒæ”¹å–„æµç¨‹ï¼Œä¸¦é—œæ³¨ç”¢æ¥­è¶¨å‹¢ï¼Œå°‡æ˜¯æå‡è‡ªæˆ‘åƒ¹å€¼æœ€æœ‰æ•ˆçš„æ–¹å¼\nKeeping a growth mindset: While the programming complexity for routine SP tasks is moderate, constantly seeking improvements and tracking industry developments is one of the fastest ways to increase oneâ€™s value.\nğŸ§©AI çš„æ‡‰ç”¨: å»¶çºŒå‰ä¸€é»ï¼ŒAI çš„æµªæ½®å·²ç¶“å¹é€²è‡¨åºŠè©¦é©—é ˜åŸŸã€‚AI ä¸åƒ…èƒ½å„ªåŒ–ç¨‹å¼é–‹ç™¼æµç¨‹ï¼Œä¹Ÿå¯èƒ½æ”¹å–„ SP çš„è¡Œæ”¿èˆ‡æºé€šæµç¨‹ã€‚ç›¸é—œè¨è«–æ­£æŒçºŒç™¼å±•ä¸­ï¼Œéå¸¸å€¼å¾—æ€è€ƒèˆ‡åƒè€ƒ\nLeveraging AI tools: Building on the previous point - the wave of AI has already entered the clinical trial space. AI has the potential to enhance both programming and administrative workflows for SPs. These discussions are ongoing, and I highly recommend staying informed and open-minded."
  }
]