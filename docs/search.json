[
  {
    "objectID": "Speaker.html",
    "href": "Speaker.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "🔹 ShinyConf 2025\n\nTitle: Reviewing Clinical Data Efficiently with Shiny\n📍 Date: 2025-04-12\n📝 Summary:\nThis presentation introduces a Shiny-based application designed to improve the efficiency of clinical data review. Traditional EDC systems often limit reviewers to viewing data one form and one patient at a time, making it difficult to cross-reference information across forms such as Adverse Events (AE), Exposure (EX), and Concomitant Medication (CM). This tool addresses that challenge by providing a user-friendly, click-driven interface that allows reviewers to select patients, filter forms and variables, and instantly visualize clinical timelines and data listings. The application maintains the original data structure, requires minimal setup by programmers, and is accessible to non-programming users. Key benefits include simultaneous multi-form data review, integrated visualizations and listings, and Excel export functionality. This tool aims to bridge communication gaps between reviewers and programmers while enhancing the speed and clarity of clinical review workflows..\n📄 Download Slides (PDF)\n\n\n\n🔹 R/Pharma 2024\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\n📍 Date: 2024-10-31\n📝 Summary:\nThis presentation explores how R and Shiny can enhance the review and visualization of clinical trial data. Traditional workflows often involve repeated back-and-forth verification between datasets such as SDTM, ADaM, and EDC, which is time-consuming. By leveraging R’s Shiny framework, we can streamline data filtering and visualization, enabling faster and more intuitive review processes for both statisticians and medical teams. The talk highlights three key Shiny applications: tumor response visualization, patient milestone tracking, and SDTM domain review. These tools support both population-level summaries and individual-level insights. Emphasis is also placed on the importance of using CDISC-compliant data formats to standardize and simplify data handling. Finally, the integration of Shiny with Quarto is introduced as a future direction to make clinical data more accessible to non-programmers, improving data transparency and efficiency in reporting.\n📄 Download Slides (PDF)\n\n\n\n🔹 Pharmasug 2018\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\n📍 Date: 2018-08-31\n📝 Summary:\nThis paper discusses how to enhance programming quality and efficiency in clinical trials under tight deadlines, especially within CROs. It emphasizes the importance of “first-time quality,” defined as the quality of deliverables before QC review. The author argues that poor initial quality leads to higher correction costs and delays, despite common practices like SOPs, validated macros, and training. A structured QC plan is essential but resource-intensive. The paper highlights factors affecting quality, including unfamiliarity with study design, insufficient task understanding, and client complexity. To address this, the author proposes a “First-Time Quality Scale” that evaluates programmers based on QC comments, client feedback, and satisfaction. High-scoring individuals can be assigned to time-sensitive projects, maximizing both quality and speed. The paper concludes that focusing on first-time quality can reduce repetitive work, save resources, and improve overall outcomes, aligning with the increasing demand for rapid and accurate clinical data processing.\n📄 Download Article (PDF)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Plot-Table Highlighting in Shiny\n\n\n\nR\n\nDT\n\nShiny\n\nplotly\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\nCollaborating with ChatGPT in Coding\n\n\n\nAI\n\nChatGPT\n\nProgramming\n\n\n\nWhat I’ve Learned So Far\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n🧧 Happy Lunar New Year! Let’s Code the Zodiac in R 🐍\n\n\n\nR\n\nhash\n\nZodiac\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\nGenerate Dynamic Text Results with glue and lapply in R\n\n\n\nR\n\nTidyverse\n\nglue\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nEfficiently Apply the Same Function to Multiple Datasets in R\n\n\n\nR\n\nlapply\n\ngsub\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "href": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "title": "Collaborating with ChatGPT in Coding",
    "section": "",
    "text": "Lately, I’ve been exploring how to work more effectively with ChatGPT when writing code—mostly in R and Python—to boost my productivity and reach my goals faster.\nTo be honest, it hasn’t always been smooth sailing. While AI can be a powerful assistant, working with it efficiently takes practice and intention. I’m still learning and fine-tuning my process, but I’ve picked up a few lessons along the way that I’d like to share. And of course, I’d love to hear your thoughts too 🪇\n💡 1. Define the Programming Goal Clearly Before asking AI for help, I’ve found it crucial to clearly explain what I’m trying to achieve. The more specific and outcome-oriented the request, the more helpful the response. When I start with a well-defined goal, ChatGPT can often propose a clean structure or even a solid template for the task.\n💡 2. Understand the AI-Generated Code This might sound obvious, but it’s tempting to copy and paste without fully understanding the AI’s output. In reality, taking the time to read and grasp the logic is essential. It helps me make sure the code aligns with my intent—and gives me a much better chance at troubleshooting if something goes wrong later on.\n💡 3. Debug Step by Step Initially, I asked ChatGPT to revise big chunks of code at once, but that often led to confusion. I’ve since learned that breaking things down into smaller parts works much better. By reviewing and applying changes step by step, I stay in control and reduce the risk of introducing new errors. It also allows me to better evaluate the AI’s reasoning behind each suggestion.\n💡 4. Restart If the Conversation Gets Stuck There are times when ChatGPT just doesn’t seem to “get it”—no matter how I phrase my question. In those cases, I’ve found it helpful to summarize the issue clearly and start a fresh conversation. A clean slate often results in clearer, more accurate responses.\n🚧 Still a Work in Progress I’m still experimenting, reflecting, and learning through this process. For me, working with AI has been more than just about writing code—it’s been a way to sharpen how I think and communicate as a developer.\nIf you’ve had similar experiences—or totally different ones—I’d love to hear how you collaborate with AI when coding. Let’s share ideas and make this learning journey smarter, together. 🔍🔍🔍"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "🧩 Introduction",
    "text": "🧩 Introduction\nWhen presenting analysis results, it’s often necessary to embed specific values into structured sentences — for example, describing results for certain patients, countries, or hospitals.\nThe glue package in R provides a powerful solution for this. It allows you to integrate variable values into a fixed text template, making your code more efficient, readable, and less prone to errors.\nIn this post, I’ll demonstrate how to: - Use glue() to dynamically create descriptive text\n- Use glue_collapse() to collapse grouped records\n- Use lapply() to manage and prefix multiple datasets\n- Integrate the results into Quarto or R Markdown reports"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "📦 Required Packages",
    "text": "📦 Required Packages\nlibrary(dplyr) \nlibrary(glue) \n🪇 Step 1: Create and Prefix Multiple Datasetsrepresenting different hospitals’ medication records:\n# Sample CM data from different hospitals\ncm1 &lt;- data.frame(USUBJID = c(\"001\", \"002\"), CMTRT = c(\"DrugA\", \"DrugB\"))\ncm2 &lt;- data.frame(USUBJID = c(\"003\", \"004\"), CMTRT = c(\"DrugC\", \"DrugD\"))\n\n# Put into a named list\ncm_list &lt;- list(Hosp1 = cm1, Hosp2 = cm2)\n\n# Prefix each dataset with its group name\ncm_prefixed &lt;- lapply(names(cm_list), function(name) {\n  df &lt;- cm_list[[name]]\n  df$Group &lt;- name\n  df\n})\n\n# Merge all into one\ncm_all &lt;- bind_rows(cm_prefixed)"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "🧠 Step 2: Collapse Records per Subject",
    "text": "🧠 Step 2: Collapse Records per Subject\nWe want to describe each subject’s treatment history by combining multiple CMTRT values:\n# Example: combine multiple treatments per subject\ncm_text &lt;- cm_all %&gt;%\n  group_by(USUBJID, Group) %&gt;%\n  summarise(cmx_CMTRT = glue_collapse(CMTRT, sep = \"; \"), .groups = \"drop\")\n✨ Step 3: Use glue() to Format Sentences\n# Create dynamic sentences for reporting\ncm_text &lt;- cm_text %&gt;%\n  mutate(sentence = glue(\"Subject {USUBJID} in {Group} was treated with: {cmx_CMTRT}.\"))\n\n# Preview\ncm_text$sentence\n📝 Output Example\nSubject 001 in Hosp1 was treated with: DrugA.\nSubject 002 in Hosp1 was treated with: DrugB.\nSubject 003 in Hosp2 was treated with: DrugC.\nSubject 004 in Hosp2 was treated with: DrugD.\n🧵 Conclusion\nThe combination of glue(), glue_collapse(), and lapply() offers a powerful workflow for:\n\nfficient text generation\nFlexible dataset processing\nClean report integration\n\nThis approach not only reduces manual effort but also ensures consistency and clarity in reporting."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "title": "🧧 Happy Lunar New Year! Let’s Code the Zodiac in R 🐍",
    "section": "",
    "text": "The Lunar New Year holiday has begun — Happy Year of the Snake! 🎉🐍🎉\nIn many Eastern cultures, the zodiac (生肖) plays an important role in tradition, storytelling, and even personal identity. This 12-year cycle assigns an animal to each year, with the animal of your birth year becoming your zodiac sign. For example, those born in 1989 or 2001 are Snakes 🐍, while others may be Tigers 🐅 or Rabbits 🐇.\nBut how exactly can we figure out someone’s zodiac animal using code?\nSure, you could just Google it… 😂\nBut as an R programmer, I decided to use this as a small coding challenge!"
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "title": "🧧 Happy Lunar New Year! Let’s Code the Zodiac in R 🐍",
    "section": "🔍 The Idea",
    "text": "🔍 The Idea\nWe’ll base our logic on the Common Era (CE) calendar, and apply a combination of vectors and the {hash} package in R to map a given year to its corresponding zodiac animal.\n\n💡 Note: For more precise results, especially if you’re working with historical or cultural data, you might want to factor in the lunar calendar (which usually starts in late January or early February). For this example, we’ll keep things simple."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "title": "🧧 Happy Lunar New Year! Let’s Code the Zodiac in R 🐍",
    "section": "🧮 How the Chinese Zodiac Works",
    "text": "🧮 How the Chinese Zodiac Works\nThe 12 animals, in order, are:\n\nRat\n\nOx\n\nTiger\n\nRabbit\n\nDragon\n\nSnake\n\nHorse\n\nGoat\n\nMonkey\n\nRooster\n\nDog\n\nPig\n\nThe cycle repeats every 12 years. For instance, the year 2025 will be the Year of the Snake, just like 2013, 2001, 1989, etc."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "title": "🧧 Happy Lunar New Year! Let’s Code the Zodiac in R 🐍",
    "section": "💻 Coding It in R",
    "text": "💻 Coding It in R\nLet’s write a simple function using the {hash} package:\n# Load the hash package library(hash)"
  },
  {
    "objectID": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "href": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "title": "Plot-Table Highlighting in Shiny",
    "section": "",
    "text": "When working with large volumes of clinical trial data, it’s easy to get lost in hundreds—or even thousands—of records. To support more intuitive data review, I’ve recently been exploring ways to combine visual plots with interactive data tables in Shiny. This approach can help reviewers quickly grasp key insights and trace them back to the raw data.\nOne idea I tested recently is a “highlight record” function, which links points on a plot to specific rows in a data listing. This functionality is built using three powerful R packages: {ggplot2}, {plotly}, and {DT}.\n🔍 How It Works: Interactive Highlighting Step-by-Step Here’s a breakdown of the mechanism:\n\n🕯️ Step 1: Create the Base Plot with ggplot2\nThe visualization starts with a standard ggplot chart—for example, plotting subject-level events by date and domain. This gives us full control over the aesthetics and data structure.\n\n\n🕯️ Step 2: Make the Plot Interactive with plotly\nNext, I use ggplotly() from the {plotly} package to convert the static ggplot into an interactive chart. With this transformation, the plot becomes clickable and dynamic, enabling deeper user interaction.\n\n\n🕯️ Step 3: Capture User Clicks Using event_data(“plotly_click”)\nThanks to plotly_click event data, I can capture exactly which point a user clicks on—such as the domain, event date, or subject ID. This click event generates metadata we can use to match against the underlying dataset.\n\n\n🕯️ Step 4: Highlight the Matched Record in DT::datatable()\nThe final step is to link the clicked point to a specific row in the data table rendered with the {DT} package. When a match is found (based on selected key values like date and domain), the corresponding row is automatically highlighted, drawing the reviewer’s attention to the source record.\n\n\n🚀 Why This Matters\nThis interaction model significantly improves the reviewer experience:\nVisual-first exploration: Users can spot patterns and anomalies visually.\nSeamless data tracing: Clicking on a point takes you straight to the corresponding record—no need to scroll through the full table.\nFaster reviews: Especially useful when reviewing patient timelines, safety events, or domain-specific findings.\n\n\n⚠️ One Caveat: Handle Factor Conversion with Care\nOne challenge I encountered involves the matching logic. Since ggplot may internally convert character variables (like domain) into factors, you must be very cautious when comparing click event values to original data values.\nA mismatch—say, due to different data types or formatting—can easily break the highlight feature. This is a common area for debugging, especially when your plot aesthetics depend on factor() transformations or custom labeling.\n\n\nFinal Thoughts\nThis approach is still evolving, but the integration of plot interactivity and table linking is already proving to be a valuable enhancement for Shiny apps, especially in clinical data review contexts. If you’re working with multi-domain datasets or timeline-based visualizations, this pattern might be worth exploring. If you have built something similar or have ideas to improve this logic, I would love to hear your thoughts! —"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "href": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "🔁 Introduction",
    "text": "🔁 Introduction\nIn data analysis, we often encounter situations where we need to perform the same transformation or calculation on multiple datasets — such as datasets grouped by age, treatment, or study phase.\nRather than repeating the same code block over and over, R provides a more efficient, less error-prone solution: the lapply() function."
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "href": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "✨ Why Use lapply()?",
    "text": "✨ Why Use lapply()?\nWhen you have more than 5 or even 10 datasets to process, using a loop or manually running the same function can quickly become tedious and risky. By placing all target datasets in a list and applying a custom function via lapply(), you can:\n\nMinimize code repetition\n\nReduce the chance of mistakes\n\nImprove code scalability and clarity"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "href": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "🔧 Example: Adjust Units and Calculate BMI",
    "text": "🔧 Example: Adjust Units and Calculate BMI\nLet’s say you have datasets containing weight (in 公斤) and height (in 公尺) for different age groups. The goal is to:\n\nConvert the units from Chinese characters to standard abbreviations (kg, m)\n\nCalculate the Body Mass Index (BMI) using the formula:\n\n\\[\n\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\n\\]\n\n📦 Step 1: Create Sample Datasets\n# Sample data for three age groups\ngroup1 &lt;- data.frame(Height = c(\"1.65公尺\", \"1.70公尺\"), Weight = c(\"60公斤\", \"65公斤\"))\n\ngroup2 &lt;- data.frame(Height = c(\"1.60公尺\", \"1.75公尺\"), Weight = c(\"55公斤\", \"70公斤\"))\n\ngroup3 &lt;- data.frame(Height = c(\"1.72公尺\", \"1.80公尺\"), Weight = c(\"68公斤\", \"75公斤\"))\n# Combine them into a list\ngroups &lt;- list(group1, group2, group3)\n🔧 Step 2: Define a Custom Function\n# Function to convert units and calculate BMI\nreplace_function &lt;- function(df) {\n  df$Height &lt;- as.numeric(gsub(\"公尺\", \"\", df$Height))  # Remove '公尺' and convert to numeric\n  df$Weight &lt;- as.numeric(gsub(\"公斤\", \"\", df$Weight))  # Remove '公斤' and convert to numeric\n  df$BMI &lt;- round(df$Weight / (df$Height^2), 1)         # Calculate BMI\n  return(df)\n}\n🚀 Step 3: Apply the Function Using lapply()\n# Apply the same function to all groups\nresults &lt;- lapply(groups, replace_function)\n\n# Preview the result for group1\nresults[[1]]\n✅ Output (example)\n  Height Weight  BMI\n1   1.65     60  22.0\n2   1.70     65  22.5"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "href": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "💡 Final Thoughts",
    "text": "💡 Final Thoughts\nThe combination of lapply() and gsub() demonstrates a powerful pattern in R: clean, consistent, and reproducible operations across datasets.\nWhether you’re dealing with different demographic groups or multiple study arms, putting your datasets in a list and defining a reusable function can save time and prevent mistakes — especially in clinical trial data preparation or large-scale reporting tasks.\nHappy coding!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Winkle Lu",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n     winklelu1226@gmail.com\n  \n\n      \nWith over a decade of experience in clinical trial programming, I specialize in CDISC standards and regulatory deliverable — but I’m not standing still. I’ve embraced open-source tools like R, Shiny, and Python to drive automation and improve data visualization.\n📘 Blog/Sharing | 👉 Presentations\n\n\nClinical Research Organization | Pharmaceutical Company | Statistical Programming\n\n\n\nMaster of Public Health | Tzu Chi University\n\n\n\n\n11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for.\n\n\n\n\n\nAllergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis\n\n\n\n\n\nWinkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time – discussion by view of first-time quality, PharmaSUG – Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428–435 [SCI]."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Winkle Lu",
    "section": "",
    "text": "Clinical Research Organization | Pharmaceutical Company | Statistical Programming"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Winkle Lu",
    "section": "",
    "text": "Master of Public Health | Tzu Chi University"
  },
  {
    "objectID": "index.html#performance-summary",
    "href": "index.html#performance-summary",
    "title": "Winkle Lu",
    "section": "",
    "text": "11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for."
  },
  {
    "objectID": "index.html#therapeutic-area",
    "href": "index.html#therapeutic-area",
    "title": "Winkle Lu",
    "section": "",
    "text": "Allergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Winkle Lu",
    "section": "",
    "text": "Winkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time – discussion by view of first-time quality, PharmaSUG – Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428–435 [SCI]."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#什麼是統計程式設計師-what-is-a-statistical-programmer-sp",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#什麼是統計程式設計師-what-is-a-statistical-programmer-sp",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "🔁 什麼是統計程式設計師 | What is a Statistical Programmer (SP)",
    "text": "🔁 什麼是統計程式設計師 | What is a Statistical Programmer (SP)\n在臨床試驗的過程中，統計程式設計師負責將試驗數據依照規範整理成可以解釋且適合報告呈現的格式。這裡所說的「規範」，包含審查主管機關（例如：FDA、EMA）所要求的提交規格、CDISC 制定的標準（如 SDTM、ADaM、define.xml），以及公司內部的作業規範。\nIn clinical trials, Statistical Programmers are responsible for organizing trial data according to predefined standards, transforming it into interpretable and report-ready formats. These “standards” include regulatory submission requirements (such as those from the FDA and EMA), CDISC-defined structures like SDTM, ADaM, and define.xml, as well as internal company-specific guidelines.\n這個問題，其實是身為統計程式設計師向身旁親友解釋工作內容時，最難說明的一部分，因為很容易與 Data Management、Statistician 等職務混淆。不過也正因如此，統計程式設計師與 Data Management、Statistician 的工作確實經常密不可分。\nThis is often one of the most difficult aspects for Statistical Programmers to explain to friends and family, as it is easily confused with roles like Data Management or Statistician. However, this overlap also highlights how closely intertwined these roles truly are.\n我在臨床試驗領域擔任統計程式設計師超過十年的時間，待過兩間大型 CRO 公司與兩間藥廠。以下我將簡單分享一些從自己角度出發，對這份工作內容與價值的觀察與理解。\nI have worked as a Statistical Programmer in the clinical trial field for over ten years, across two large CRO companies and two pharmaceutical firms. In the following sections, I’ll share my perspective on the nature and value of this profession based on my own experiences."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-在做什麼-what-does-a-statistical-programmer-actually-do",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-在做什麼-what-does-a-statistical-programmer-actually-do",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "🔁 Statistical Programmer 在做什麼 | What Does a Statistical Programmer Actually Do",
    "text": "🔁 Statistical Programmer 在做什麼 | What Does a Statistical Programmer Actually Do\n先前擔任面試主管的時候，常會聽到面試者提到 CDISC。CDISC 是什麼呢？簡單來說，是針對臨床數據制定相關交換與整理標準的組織。而其中 SDTM、ADaM 的標準化規範與建議就是由 CDISC 制定出來的。這其實就是 SP 核心工作的重要標準之一。\nWhen I previously served as an interview manager, I often heard candidates mention CDISC. So, what exactly is CDISC? Simply, it is an organization that defines data exchange and standardization frameworks for clinical trial data. The well-known SDTM and ADaM standards were established by CDISC - and these are among the key foundations of a Statistical Programmer’s role.\nCDISC 是我非常喜歡的單位之一，因為臨床原始數據其實是非常多元的。這個組織針對臨床試驗各階段的數據進行標準化整理，讓資料在整理、分析、以及提交方面更具效率。\nCDISC is one of my favorite organizations, because clinical raw data is inherently diverse and complex. By applying standardized structures to each phase of a clinical trial, CDISC enables much more efficient data organization, analysis, and regulatory submission.\n\n以一般的流程來說，當臨床數據依照 CRF（Case Report Form）進行收集至 EDC（Electronic Data Capture）系統後，SP 將依照案子的進度與需求，開始進行 SDTM 數據集的程式編寫。同時間，也會與相關部門，例如統計與醫學部門，確認統計分析的內容後，開始進行 ADaM 及 TLF（Table, Listing, Figure）的準備。\nIn a general workflow, once clinical data is collected by CRFs (Case Report Forms) and entered into the EDC (Electronic Data Capture) system, SPs begin programming the SDTM datasets based on the project timeline and needs. At the same time, they collaborate with key departments - such as statisticians and medical reviewers - to confirm the analysis plan and begin developing ADaM datasets and generating TLFs (Tables, Listings, and Figures).\n回憶自己剛進這領域時，那時候的部門分工十分細，SDTM 與 ADaM+TLF 是由不同的 SP 團隊負責。我一開始是在負責 ADaM+TLF 的團隊，當時第一個主要任務是 ADLB 數據集。由於數量龐大、分析方法複雜，程式每次執行都要耗時 4～5 小時。\nLooking back to when I first entered the field, the departments were highly specialized - separate teams handled SDTM and ADaM+TLF. I started in the ADaM+TLF team, and my first major task was the ADLB (laboratory) dataset. Due to its size and the complexity of the analysis methods, each run of the program would take 4 to 5 hours.\n記得那時候團隊常常一起晚餐後又一起加班，就這樣持續了幾個月，最後順利完成 Sponsor 的需求，接著也推進至 define.xml 的製作。對當時年資還不到一年的我來說，能接觸這樣的任務實屬難得，哈哈。\nI remember how the team often stayed late after dinner to keep working together. After several months, we successfully met the sponsor’s expectations and moved on to preparing define.xml. At the time, I had less than one year of experience, so being involved in such tasks was rare - and rewarding - for a junior SP.\n雖然第一年的 SP 生活讓我吃盡苦頭，不過在當時 mentor 和團隊的帶領下，也讓我更快打下 SP 生涯的重要基礎。\nAlthough that first year as a Statistical Programmer was filled with challenges, the guidance of my mentor and the support of the team helped me quickly build a solid foundation in this career."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#sp-與現在的臨床試驗-statistical-programmers-in-todays-clinical-trials",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#sp-與現在的臨床試驗-statistical-programmers-in-todays-clinical-trials",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "✨ SP 與現在的臨床試驗 | Statistical Programmers in Today’s Clinical Trials",
    "text": "✨ SP 與現在的臨床試驗 | Statistical Programmers in Today’s Clinical Trials\n剛入行時，SP 工作多數以 SAS 為主要的常用程式語言。SAS 是一項非常穩定的工具，我認為最強大的部分是它背後的支持公司資源。即使遇到 SAS 本身的技術問題，也會有專業團隊協助解決；另外，由於這是臨床試驗領域長久以來所使用的語言，可供參考的程式範例非常豐富，這也是至今 SAS 擁有不可撼動地位的原因之一。\nWhen I first entered the field, SAS was the dominant programming language for Statistical Programmers. It is a highly stable tool, and in my view, its greatest strength lies in the robust support from the company behind it. Even when technical issues arise, professional support teams are available to help. Moreover, as SAS has been widely used in clinical trials for decades, it offers an abundance of reference programs — one of the key reasons for its long-standing, unshakable position in the field,\n印象中，從 2018 年開始，我開始有機會參與公司外的研討會，open-source tool 相關的分享已經很多，例如：R、Python。這是非常好的現象，代表整個臨床試驗領域正在不斷思考與進步。對我來說，這些工具不應該被用來與 SAS 相互比較或競爭，而是提供給使用者（例如 SP）更多協作的可能性與思考的空間。\nI recall that starting around 2018, I had the opportunity to attend external conferences, and there were already many discussions around open-source tools like R and Python. This is a very positive trend - it shows that the clinical trial industry is continuously evolving and open to new ideas. Personally, I don’t see these tools as competitors to SAS. Rather, they offer more collaborative options and new perspectives for users like Statistical Programmers."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#至於什麼是現在-sp-的利器-what-are-the-current-must-have-skills-for-sps",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#至於什麼是現在-sp-的利器-what-are-the-current-must-have-skills-for-sps",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "💡 至於什麼是現在 SP 的利器 | What Are the Current Must-Have Skills for SPs",
    "text": "💡 至於什麼是現在 SP 的利器 | What Are the Current Must-Have Skills for SPs\n如果由我來回答，我會列舉以下幾點：\nIf I were to answer this question, here are the capabilities I consider essential:\n🧩理解當下所進行的任務: SP 經手許多任務也常會跟其他部門合作, 理解當下任務的原由除了可以幫助維持正確達成目標外, 也可以適時提出更合適的解決方案\nUnderstanding the task at hand: SPs handle various tasks and frequently collaborate with other departments. Understanding the purpose behind the current task not only helps ensure that the objectives are met correctly, but also allows SPs to propose more appropriate solutions when needed.\n🧩維持高品質的產出: 臨床試驗直接關乎人體生命，是一份需要審慎對待的工作；若因品質不佳而多次修改，將直接影響案子的整體時程\nDelivering high-quality outputs: Clinical trials involve human lives, and this work must be approached with great care. Poor quality that requires repeated revisions can directly delay project timelines.\n🧩熟悉常規工作的技能: 了解每個常規步驟，並高效率完成工作，是降低風險與提升穩定性的關鍵\nMastering routine tasks: Understanding each standard step and performing them efficiently helps reduce risk and ensure consistent deliverables.\n🧩維持思考與拓展視野: SP 的常規程式開發需求屬於中等程度，但若能不斷思考改善流程，並關注產業趨勢，將是提升自我價值最有效的方式\nKeeping a growth mindset: While the programming complexity for routine SP tasks is moderate, constantly seeking improvements and tracking industry developments is one of the fastest ways to increase one’s value.\n🧩AI 的應用: 延續前一點，AI 的浪潮已經吹進臨床試驗領域。AI 不僅能優化程式開發流程，也可能改善 SP 的行政與溝通流程。相關討論正持續發展中，非常值得思考與參考\nLeveraging AI tools: Building on the previous point - the wave of AI has already entered the clinical trial space. AI has the potential to enhance both programming and administrative workflows for SPs. These discussions are ongoing, and I highly recommend staying informed and open-minded."
  }
]